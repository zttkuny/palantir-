# Palantir Foundry 平台技术调研报告

## 一、平台概述

### 1.1 平台定位

Palantir Foundry 是 Palantir 公司开发的企业级数据整合与分析平台，提供从数据接入到应用开发的全生命周期数据科学解决方案。平台以**本体（Ontology）**为核心架构，支持低代码/无代码开发模式，同时提供代码级扩展能力，满足不同技术背景用户的需求。

### 1.2 核心设计理念

- **本体驱动架构**：将数据转化为业务对象，建立数字孪生模型
- **低代码优先**：通过图形化界面实现复杂数据操作，降低技术门槛
- **数据血缘完整**：全程维护数据出处，确保可追溯性和可重现性
- **AI 深度集成**：内置 AI 助手和大模型能力，支持智能数据处理和应用开发

### 1.3 技术栈组成

Foundry 平台包含多个核心应用模块，覆盖数据处理、分析、建模和应用开发的完整链路：

```
数据接入 → 数据处理 → 数据建模 → 数据分析 → 应用开发 → 自动化运维
   ↓          ↓          ↓          ↓          ↓          ↓
Data      Pipeline    Ontology   Contour/   Workshop   Automate
Connection Builder              Quiver
```

---

## 二、核心功能模块详解

### 2.1 数据接入与集成（Data Connection）

#### 2.1.1 架构设计

数据连接采用 **Source（源）** 和 **Sync（同步）** 的架构模式：

- **Source（源）**：存储连接外部系统的指令，包括 URL、端口号、安全凭据（API 密钥、OAuth 令牌等）。内置机密管理器，确保敏感信息加密存储。
- **Sync（同步）**：定义从特定 Source 中提取哪些具体数据的指令（如特定的表、文件夹或文件）。一个 Source 可以有多个 Sync，形成一对多关系。

#### 2.1.2 支持的连接类型

**文件系统连接（AWS S3）**
- 连接到云端对象存储（S3、Azure Blob、Google Cloud Storage）
- 支持 CSV、Parquet、JSON 等多种文件格式
- 自动模式检测（Schema Detection），将文件转化为表格数据集

**关系型数据库连接（PostgreSQL）**
- 通过 JDBC 协议连接数据库
- 支持 SSL 证书配置，确保安全连接
- 使用 SQL 查询提取数据，对熟悉 SQL 的用户友好

**REST API 连接**
- 在 Data Connection 中创建源配置
- 在 Code Repositories 中使用 Python 编写转换逻辑
- 支持源基外部转换（Source-based external transforms），简化配置维护

#### 2.1.3 安全机制

**Egress Policy（出口策略）**
- Foundry 默认锁定所有出站请求，确保安全性
- 出口策略作为防火墙许可，控制允许访问的外部地址
- 需要信息安全官（ISO）角色权限配置

**连接方式选择**
- **直接连接**：用于可公网访问的数据源，优势在于无需维护额外软件，性能优异
- **代理（Agents）**：用于位于内网或防火墙后的数据源，通过单向加密连接确保安全

#### 2.1.4 自动化与监控

- **计划任务（Schedule）**：设置同步频率（如每天凌晨自动更新）
- **数据质量检查**：为同步任务添加数据质量检查规则
- **数据血缘追溯**：在 Data Lineage 应用中可视化端到端数据流转路径

---

### 2.2 数据管道构建（Pipeline Builder）

#### 2.2.1 工具定位

Pipeline Builder 是 Foundry 的无代码/低代码数据管道构建工具，与 Code Repositories（Pro-code 方案）形成互补。支持将原始数据转化为清洗后的数据资产，最终生成能够支撑本体、分析或模型的"黄金数据"。

#### 2.2.2 核心功能特性

**多格式数据接入**
- 支持 CSV、Parquet（针对 Spark 优化的列式存储）、JSON 等格式
- 对于 JSON 等无模式数据，可自动提取行信息、解析字典结构并生成表格式视图

**数据清洗与转换**
- **基础转换**：
  - Trim Whitespace：去除首尾空格，确保字段一致性
  - Cast：类型转换（如将字符串格式的价格转换为 Double 类型）
  - Filter：移除包含空值或无效数据的行
  
- **复杂结构处理**：
  - Flatten Struct：展平嵌套的 JSON 结构体
  - Concatenate：拼接字符串，将分散字段组合成易读格式（如地址字段）
  
- **AI 助手集成（AIP Assist）**：
  - 通过自然语言描述生成复杂转换逻辑
  - 自动识别并生成时间戳格式字符串
  - 极大简化开发难度，减少查阅技术文档的工作

**数据集成与连接**
- **多表连接（Joins）**：支持左连接、右连接等多种连接方式
- **问题排查**：使用 View Stats 功能识别"连接爆炸"或数据重复问题
- **连接键优化**：通过更换连接键（如使用 Variation ID 替代 Product ID）修复逻辑错误

**业务逻辑派生**
- **衍生指标计算**：通过乘法转换计算收入（单价 × 数量）
- **数据聚合**：使用 Group By 操作计算客户终身价值（CLV）等业务指标

#### 2.2.3 部署与运维

**实例化输出（Materializing Outputs）**
- Pipeline Builder 中的节点只是中间结果
- 必须添加"新数据集"输出并执行部署（Deploy）动作
- 数据才会在平台中实际构建并可用

**管道分层架构（Pipeline Segmentation）**
建议按照以下阶段对管道进行拆分：
- **Raw（原始层）**：原始数据接入
- **Clean（清洗层）**：数据清洗和基础转换
- **Enriched（丰富层）**：业务逻辑派生和聚合
- **Ontology（本体层）**：发布到本体模型

这种分层设计提高可读性和可维护性。

**数据质量监控（Data Expectations）**
- 设置主动监控检查（如检查主键唯一性、行数范围）
- 不符合规则时可触发警告或中断构建
- 防止坏数据传播到下游系统

**增量转换（Incremental Transforms）**
- 对于大规模数据，仅处理新流入的行
- 减少计算负载并提高处理速度

#### 2.2.4 作业追踪（Job Tracker）

通过 Job Tracker 监控后台 Spark 作业的执行进度和成功状态，确保数据管道的稳定运行。

---

### 2.3 AI/LLM 集成能力

#### 2.3.1 RAG 工作流（检索增强生成）

**核心概念**
RAG 让大语言模型（LLM）能够访问其训练数据之外的特定知识库（如公司的 PDF 手册），生成更准确、有据可查的回答，避免模型幻觉。

**完整流程**

1. **PDF 文本提取**
   - 将 PDF 文件上传为 Media Set（媒体集）资源
   - 提取方法选择：
     - **Raw Text**：适用于机器可读的 PDF
     - **OCR**：适用于扫描件、照片或非直接可读的 PDF
     - **Layout Aware**：用于处理包含表格、图像等复杂布局的文档
   - 支持指定提取的页码范围

2. **字符串整合（Joining Array）**
   - 提取的文本最初以数组形式存储，每个元素代表一页
   - 使用 Join Array 将多页内容合并为单一字符串
   - 防止语义在翻页时被截断

3. **文本分块（Chunking）**
   - 将长文档切分为更小、更易管理的片段
   - 必要性：
     - 嵌入模型和 LLM 都有上下文窗口限制
     - 检索精度：返回最相关的"片段"而非整本文档
   - 配置：使用 Chunk String 节点，设置分块大小为 512 tokens，根据段落、句子和单词边界智能切分

4. **计算向量嵌入（Compute Embeddings）**
   - 将文本转换为高维空间的向量数字序列，捕捉文本语义
   - 语义搜索原理：通过比较向量距离（如余弦相似度）识别语义相近概念
   - 模型对齐：选择预训练模型（如 text-embedding-ada-002）
   - **关键原则**：必须同时保留原始文本（用于生成回答）和向量数据（用于检索），且后续应用必须使用与生成阶段一致的嵌入模型

5. **本体化与应用集成**
   - 创建"Chunk（分块）"对象类型，将分块编号设为主键
   - 将嵌入字段配置为 Vector 类型
   - 下游可被 AIP Logic 或 AIP Agent Studio 调用，构建实时问答智能体应用

#### 2.3.2 LLM 实体提取

**Use LLM 节点**
Pipeline Builder 中的核心功能节点，预设六种模板：
- 分类
- 情感分析
- 摘要
- **实体提取**：从文本中提取特定元素（客户姓名、项目名称、日期等）
- 翻译
- 空提示词（自定义）

**工作流程**

1. **数据接入**：将 PDF 上传为 Media Set，可直接将媒体引用传入 LLM 节点，无需预先提取文本

2. **配置提取逻辑**：
   - 上下文定义：设定提取的背景信息（如地热能源项目）
   - 定义实体类型：明确提取目标（如"地点"、"项目"、"引用论文"）
   - 模型选择：选择模型（Gemini 2.0 Flash 或 GPT-4）
   - 系统自动生成系统提示词和任务提示词

3. **数据结构化**：
   - 将输出类型从 String 改为 Array，获取文档中所有相关实体
   - 使用"Extract many struct fields"转换操作，将提取结果转化为独立数据列
   - 部署为标准数据集，用于后续分析或作为本体对象支撑

**应用价值**
- 替代人工阅读，快速获取关键信息点
- 将杂乱文本信息转化为结构化、可搜索字段
- 支持后续统计分析和知识图谱构建

---

### 2.4 本体建模（Ontology）

#### 2.4.1 核心概念

本体是 Foundry 组织结构化数据的核心，将原始数据集转化为易于理解的业务分类（对象类型）。它是构建上层应用程序（如 Workshop）和工作流的"数字孪生"基础。

**映射关系**
- 一个数据集对应一个对象类型（Object Type）
- 数据集中的一行对应一个对象实例（Instance）
- 数据集中的一列对应一个对象属性（Property）

#### 2.4.2 对象类型定义

**关键标识**
- **主键（Primary Key）**：每个对象必须有一个唯一且不可重复的主键（如患者 ID），用于区分不同实例
- **标题键（Title Key）**：用于在界面上以人类可读的方式显示对象名称（如患者姓名），通常不需要像主键那样必须是哈希字符串

**创建方式**
- **通过本体管理器（Ontology Manager）**：配置元数据、主键、标题键以及权限，生成操作类型（Action Types）
- **通过管道构建器（Pipeline Builder）**：在数据处理完成后，直接在输出端将其定义为"新对象类型"并部署

#### 2.4.3 链接关系（Links）

链接用于反映现实世界中实体间的复杂关系：

**一对多（1:Many）**
- 一名患者可以经历多次手术
- 使用外键关联

**一对一（1:1）**
- 一次手术仅对应一个唯一的预后结果
- 直接关联

**多对多（Many:Many）**
- 多名患者可以接受同一种治疗类型，且一种治疗类型可应用于多名患者
- **注意**：多对多链接必须由一个连接表（Join Table）支持

#### 2.4.4 本体函数（Ontology Functions）

通过编写 TypeScript 函数扩展本体功能，支持在 Workshop 应用中实现复杂业务逻辑。

**函数类型**

1. **基础指标计算函数（@Function）**
   - 使用 groupBy 方法按维度聚合数据
   - 计算业务指标（如 DSO - 应收账款周转天数）
   - 在 Workshop 中创建函数驱动的数值变量，填充指标卡

2. **函数驱动的衍生列（Function-backed Column）**
   - 创建映射（Map），将对象与其计算值关联
   - 在 Workshop 的对象表格中添加衍生列
   - 支持数值格式和条件格式（如 DSO > 40 显示红色）

3. **动态图表聚合函数**
   - 计算聚合结果（如月度 DSO 变化）
   - 在 Workshop 中配置 XY 图表，数据源使用函数
   - 图表根据用户选择实时更新趋势

4. **函数驱动的操作逻辑（@OntologyEditFunction）**
   - 使用 @OntologyEditFunction 装饰器
   - 不仅计算数据，还能修改本体对象
   - 在 Ontology Manager 中包装成操作（Action），在 Workshop 中通过按钮触发

**技术特性**
- **装饰器**：@Function 暴露读取逻辑，@OntologyEditFunction 暴露数据修改逻辑
- **异步处理**：使用 TypeScript 的 Promise（如 Promise.all）并行处理，提高性能
- **实时预览**：在代码存储库中使用 Live Preview 功能，预览函数运行结果

#### 2.4.5 验证与探索工具

**对象浏览器（Object Explorer）**
- 搜索并筛选特定对象（如筛选患有"肥胖症"的患者）
- 路径追踪：从患者跳转到关联的手术，再跳转到预后结果
- 快速获取业务洞察

**数据血缘（Data Lineage）**
- 可视化追踪数据从原始源头到处理步骤，再到最终本体对象及其链接关系的完整路径
- 确保数据的可追溯性

---

### 2.5 数据分析工具

#### 2.5.1 Contour（数据集分析工具）

**工具定位**
Contour 是 Foundry 中的低代码/无代码数据分析工具，专门用于处理数据集（Data Sets）。当数据尚未进入本体（Ontology），且用户更倾向于非代码操作而非 Python/R 编程时，Contour 是最佳选择。

**核心架构：Paths & Boards**

**路径（Paths）**
- 类似于 Excel 里的工作表
- 一个分析可以由多个路径组成，每个路径有特定用途（如数据清洗、可视化或查找表）
- 新路径可以从原始数据集开始，也可以从现有分析的中间结果开始

**板（Boards）**
- 分析的基本构建块，在路径内按顺序应用
- 线性逻辑：上方的板会影响下方的板，但反之则不然
- 路径依赖性：一个路径的起点可以是另一个路径的输出，允许在主路径上应用全局过滤

**数据清洗与准备**
- **Calculation Board**：计算汇总统计数据（如唯一计数）
- **Multi-column Editor**：重命名或删除不必要的列
- **Find and Replace**：清除字符串中的杂质（如去除前缀 p-）
- **Convert Types**：类型转换（如将字符串字段转换为 Double）
- **Filter**：按日期范围或特定条件筛选行

**数据集成（Joins）**
- 在路径中创建"Parts and Equipment join"
- 将不同数据集连接，添加前缀避免列名冲突
- 设置匹配条件（如设备 ID）

**高级逻辑处理（Expression Board）**
- 编写类似 SQL 的逻辑创建衍生列
- **开窗函数（Window Functions）**：使用 RANK 或 DENSE_RANK 进行排名
- **CASE WHEN 逻辑**：根据条件生成风险等级（如高、中、低警告）

**可视化与参数化**
- **参数（Parameters）**：创建参数变量（如"工厂名称"），链接到过滤器板
- **动态交互**：用户通过下拉菜单动态调整显示的数据范围，无需修改后台逻辑
- **图表类型**：
  - 直方图（Histogram）：支持交叉过滤，点击条柱过滤下游所有数据
  - 透视表（Pivot Table）：按维度分组统计，可格式化单元格数据
  - 散点图与条形图：支持堆叠（Stacked）或分组（Grouped）显示

**高级报表功能**
- **表达式板（Expression Boards）**：编写 SQL 片段（CASE 语句、CONCAT 拼接）
- **分箱与分段（Bucketing/Binning）**：将连续数值划分为区间，实现颜色标记
- **图表叠加（Overlays）**：添加叠加层，将面积图和线条图合并
- **颜色自定义**：使用 Hex 颜色代码手动配置图表颜色

**看板发布**
- **Dashboard**：从各个路径中精选关键板，"添加到仪表板"
- 布局调整和版本管理
- 参数在侧边栏显示，供非技术用户进行筛选交互
- 结果可导出为 PDF，或复制到 Notepad 应用

#### 2.5.2 Quiver（本体数据分析工具）

**工具定位**
Quiver 是 Foundry 中专门用于分析本体（Ontology）对象的工具。当处理的是已经建模的本体对象，尤其是带有时间序列分量的大规模数据时，Quiver 是首选工具。

**与 Contour 的对比**
- **Contour**：处理原始数据集（Data Sets），尚未对象化
- **Quiver**：处理本体对象，支持大规模对象数据和时间序列

**基础架构**

**卡片（Cards）**
- Quiver 的构建块，每张卡片执行特定操作（输入/输出）
- 拥有全局 ID（如 $A）以便在公式中引用

**视图模式**
- **画布视图（Canvas View）**：直观、可排版的视图，方便组织组件位置和大小
- **图表视图（Graph View）**：显示分析逻辑的全景流向图，追踪数据来源和转换过程

**数据准备与转换**

**对象集成（Joins）**
- 使用转换表（Transform Tables）将不同对象集进行左连接
- **技术限制**：转换表的操作上限通常为 50,000 条记录，超过需使用物化（Materializations）技术

**数据清洗**
- Find and Replace：移除字符串前缀
- Cast：将清洗后的字符串转换为数字类型

**交互式参数（Parameters）**
- 创建参数化过滤器（如工厂选择下拉菜单）
- 不同用户在查看看板时能动态筛选数据，不会相互干扰

**高级分析与逻辑派生**

**聚合操作（Group By）**
- 按对象维度进行分组，对数组进行聚合计算（如计算平均纯度）

**公式计算**
- 通过 Numeric Formula 编写逻辑
- 例如：用"实际产量 / 设备产能"计算"实际产出百分比"

**视觉函数（Visual Functions）**
- Quiver 的核心进阶功能
- 将复杂的分析逻辑封装成可复用的函数
- 同事们可直接调用该函数对不同的对象集执行相同分析，无需重新构建逻辑

**可视化与看板策划**

**标准图表**
- 包括柱状图、散点图等

**Vega 图表（Vega Plot）**
- 对于内置图表无法满足的复杂需求
- 使用 Vega-Lite 语法创建高级可视化（如箱线图 Box Plot）

**看板发布**
- 将选定的卡片"添加到看板"，添加标题、注释和交互控件
- 分析结果可以复制到 Notepad 生成报告，或嵌入到 Workshop 应用程序中

---

### 2.6 应用开发（Workshop）

#### 2.6.1 工具定位

Workshop 是 Foundry 的低代码应用开发环境，允许开发者通过简单的点击和配置（Point-and-click），而非编写大量代码，来构建复杂的企业级应用。应用完全由本体中的**对象（Objects）、链接（Links）和操作（Actions）**驱动。

**与数据看板的区别**
不同于单纯展示数据的仪表板（如 Contour），Workshop 旨在创建**操作型应用（Operational Applications）**，让用户能够在探索数据的同时做出决策并直接修改底层系统状态。

#### 2.6.2 核心概念

**变量驱动逻辑（Variables）**
- 变量是 Workshop 内部数据流动的控制核心
- **对象集变量**：存储一组对象（如"所有货物"）
- **变量链（Variable Chaining）**：实现组件间的复杂联动（如点击过滤器后，图表和表格同步更新）

**Marketplace 安装**
- 从 Marketplace（Foundry 的数据产品商店）安装预配置的本体包
- 包含对象类型、数据集和链接类型，快速启动项目

#### 2.6.3 交互式组件构建（Widgets）

**过滤器（Filter List）**
- 按维度筛选对象（如按起始地、目的地或日期筛选货物）

**XY 图表（Chart XY）**
- 展示数据分布，支持多层数据展示（如同时显示货物数和产品量）

**对象表格（Object Table）**
- 显示详细的对象信息清单
- 支持添加函数驱动的衍生列

**甘特图（Gantt Chart）**
- 直观展示时间表（如货运的发运和到达时间）
- 帮助识别时间重叠且有合并潜力的订单

#### 2.6.4 实现业务逻辑（Actions & Functions）

**操作（Actions）**
- 在本体管理器中定义"标记为待合并"等动作
- 作为用户与数据交互的"动词"
- 在 Workshop 中通过按钮触发，直接修改底层本体对象的状态

**TypeScript 函数**
- 当标准 Action 无法满足需求时（如需要多选并一次性处理多个对象）
- 编写逻辑函数来支撑 Action
- 使用 @Function 或 @OntologyEditFunction 装饰器

#### 2.6.5 布局与美化

**页面结构**
- 使用页面（Page）、层级（Sections）和内距（Padding）优化视觉体验
- 紧凑填充（Compact Padding）、无边框设计（Borderless sections）

**条件显示**
- 通过布尔变量控制组件可见性
- 例如：仅当用户切换到特定的"甘特图"标签页时，才显示"标记合并"按钮

#### 2.6.6 数据血缘与可追溯性

通过 Data Lineage（数据血缘），用户可以清晰地看到从原始数据集到本体对象，再到最终 Workshop 分析看板的完整链路。这确保了应用的数据来源清晰透明。

---

### 2.7 代码存储库（Code Repositories）

#### 2.7.1 工具定位

Code Repositories 是 Foundry 中的 Pro-code 开发环境，适合偏好代码的开发者。与 Pipeline Builder（无代码方案）形成互补，使用 PySpark 结合 Foundry 的 API 编写转换逻辑。

#### 2.7.2 开发环境特性

- **基于 Web 的集成开发环境（IDE）**：具备 Git 集成、计算配置文件（Compute Profiles）访问等常见 IDE 功能
- **技术栈**：主要使用 PySpark 结合 Foundry 的 API

#### 2.7.3 核心开发模式

**环境初始化**
- 创建专门的项目和文件夹结构（如 logic、data/raw、data/prepared）
- 初始化 Python 转换存储库
- 通过资源 ID (RID) 或文件系统路径引用输入数据集

**核心转换操作**
- **数据清洗**：使用正则表达式清除字符串杂质，将日期字段转换为标准格式
- **数据过滤**：利用 PySpark 函数根据布尔值列筛选有效行
- **多表关联**：通过 transform_df 装饰器引入多个输入，执行左连接
- **数据聚合**：使用 groupBy 方法计算业务指标

**代码发布与构建**
- **预览（Preview）**：在不写入磁盘的情况下查看转换后的数据样貌
- **提交（Commit）**：保存代码状态并触发持续集成（CI）检查
- **构建（Build）**：启动 Spark 作业执行逻辑，在 Foundry 文件系统中实例化输出数据集

#### 2.7.4 协作与分支管理

**分支保护（Branch Protection）**
- 将 master 分支设为只读，保护生产级代码

**分支开发流程**
- 从主分支切出新分支进行功能开发
- 通过合并请求（Pull Request）描述变更
- 利用"压缩并合并（Squash and merge）"等 Git 技术将代码合并回主干

#### 2.7.5 数据血缘与可追溯性

- 在数据血缘应用中观察从原始 CSV 到聚合报告的全路径
- 对于由代码生成的资产，用户可以在 Data Lineage 中直接查看其背后的 PySpark 逻辑

---

### 2.8 数据治理与保护

#### 2.8.1 三大核心工具

**Checkpoints（检查点）**
- **核心定义**：交互式提示，当用户在 Foundry 中执行敏感操作（如导出数据、下载文件）时，系统强制要求用户提供操作理由或合规性确认
- **配置与触发**：管理员可以根据命名空间、特定用户组或数据标记（Marking）设置触发范围
- **理由类型**：支持确认勾选（Acknowledgement）、下拉菜单选择原因、文本回复或重新进行身份验证
- **审计日志**：系统会记录这些日志，以便后续审计

**Sensitive Data Scanner（敏感数据扫描器）**
- **核心定义**：自动扫描并标记项目中的敏感信息（如个人身份信息 PII），避免依赖人工检查庞大的数据集
- **匹配逻辑**：
  - 支持平台预设的条件（如电子邮件、社保号 SSN、电话号码等）
  - 通过正则表达式（RegEx）自定义匹配规则（如匹配"婚姻状况"等特定字段）
- **自动化响应**：扫描发现敏感数据后，系统可以执行自动操作（如创建问题单或自动应用安全标记）

**Cipher（加密与脱敏）**
- **核心定义**：Foundry 的核心混淆工具，用于对敏感字段进行加密处理，支持可逆加密和不可逆混淆
- **管理机制**：
  - **Cipher Channel（频道）**：定义加密算法和加密系统（如确定性加密）
  - **Cipher License（许可证）**：充当访问密钥。管理员可以分别针对数据集操作或本体操作创建许可证，严格控制谁能加密或解密特定数据
- **应用场景**：在 Pipeline Builder 中对员工姓名等字段进行加密，使数据在物理存储中显示为密文。在应用层（如 Object Explorer），只有拥有相应解密许可证的用户才能查看原始值

#### 2.8.2 治理角色

要配置和管理这些工具，用户通常需要在 Foundry 中具备 **Data Governance Officer（数据治理专员）** 角色。

#### 2.8.3 综合应用

这些工具通常与 **Markings（安全标记）** 结合使用，共同实现"数据最小化"原则，确保只有获得授权的人员在合规的场景下才能接触敏感数据。

**完整数据保护链路**：从自动检测风险（Scanner）、到操作合规约束（Checkpoints）、再到物理数据加密（Cipher）的完整数据保护链路。
